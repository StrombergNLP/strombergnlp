---
title: "Hyperparameter Power Impact in Transformer Language Model Training"
authors:
- lupu
- mkon
- timn
- admin
date: "2021-07-01T00:00:00Z"
doi: ""
publication_types: ["1"]
publication: "Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing"
abstract: "Training large language models can consume a large amount of energy. 
We hypothesize that the language model's configuration impacts its energy consumption, and that there is room for power consumption optimisation in modern large language models.
To investigate these claims, we introduce a power consumption factor to the objective function, and explore the range of models and hyperparameter configurations that affect power.
We identify multiple configuration factors that can reduce power consumption during language model training while retaining model quality."
summary: 


tags:
#- Source Themes
#featured: true

links:
#- name: Custom Link
#  url: http://example.org
url_pdf: 'http://www.derczynski.com/papers/hpitlmt.pdf'
url_code: 'https://github.com/mkonxd/HyperparameterPowerImpact'
#url_dataset: '#'
#url_poster: '#'
#url_project: ''
#url_slides: ''
#url_source: '#'
#url_video: '#'

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
image:
#  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/pLCdAaMFLTE)'
  focal_point: ""
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects:
- efficient

---
